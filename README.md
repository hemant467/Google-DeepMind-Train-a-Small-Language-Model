# Google DeepMind Train a Small Language Model ğŸ’ 
Character-level Arabic language model ğŸ¤– for generating simple, coherent stories using n-gram and transformer-based approaches.

# Overview ğŸ“œ :

Cymbal Chat is a startup-focused project designed to develop a character-level language model for generating simple, coherent stories in Arabic. This repository contains the foundational tools and data preparation framework for training custom, small-scale transformer models for Arabic NLP tasks.

Character-based models can offer better performance than word-based models for languages like Arabic, where morphology and word composition are complex. This project provides a starting point for creating robust, specialized language models tailored to Arabic text generation.

# Features âœ¨ :

- Character-level Tokenizer: Efficiently encodes Arabic text at the character level for model training.
- N-gram Text Generator: Provides a baseline generative model for Arabic story generation.
- Data Preparation Pipeline: Prepares raw Arabic text for training character-based transformer models.
- Extensible Codebase: Easy to extend for custom datasets and advanced model architectures.

# Project Structure ğŸ—ï¸ :
```text
cymbal-chat-arabic/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Original Arabic story datasets
â”‚   â””â”€â”€ processed/      # Tokenized and cleaned datasets ready for training
â”‚
â”œâ”€â”€ notebooks/          # Jupyter notebooks for experimentation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tokenizer.py     # Character-level tokenizer implementation
â”‚   â”œâ”€â”€ ngram_model.py  # N-gram text generator
â”‚   â””â”€â”€ prepare_data.py # Data preparation pipeline
â”‚
â”œâ”€â”€ scripts/            # Training and evaluation scripts
â”‚
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # Project documentation
```

Usage
1. Prepare the Dataset

Place your Arabic story dataset in data/raw/ and run the preprocessing script:

```bash
python src/prepare_data.py --input_dir data/raw --output_dir data/processed
```

2. Train the N-gram Baseline Model ğŸ¤–

```bash
python src/ngram_model.py --data_path data/processed/stories.txt --n 3
```

3. Tokenize Text for Transformer Model ğŸ¤–

```bash
python src/tokenizer.py --input data/processed/stories.txt --output data/processed/tokenized.npy
```

<img src="https://readme-typing-svg.herokuapp.com/?lines=Google+DeepMind+Train+a+Small+Language+Model+ğŸ’ &font=Fira%20Code&color=%23FFD700&center=true&width=600&height=60">
